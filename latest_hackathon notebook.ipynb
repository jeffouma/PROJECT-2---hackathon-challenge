{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HACKATHON CHALLENGE","metadata":{}},{"cell_type":"markdown","source":"Will start by importing the neccessary packages that will be used for the data processing.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:29.970798Z","iopub.execute_input":"2022-06-26T10:52:29.971663Z","iopub.status.idle":"2022-06-26T10:52:30.004812Z","shell.execute_reply.started":"2022-06-26T10:52:29.971530Z","shell.execute_reply":"2022-06-26T10:52:30.003795Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom string import punctuation\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:30.198848Z","iopub.execute_input":"2022-06-26T10:52:30.199743Z","iopub.status.idle":"2022-06-26T10:52:31.682623Z","shell.execute_reply.started":"2022-06-26T10:52:30.199705Z","shell.execute_reply":"2022-06-26T10:52:31.681575Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/south-african-language-identification-hack-2022/train_set.csv')\ntest = pd.read_csv('/kaggle/input/south-african-language-identification-hack-2022/test_set.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:07:05.367893Z","iopub.execute_input":"2022-06-26T11:07:05.368847Z","iopub.status.idle":"2022-06-26T11:07:05.508637Z","shell.execute_reply.started":"2022-06-26T11:07:05.368786Z","shell.execute_reply":"2022-06-26T11:07:05.507228Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS","metadata":{"execution":{"iopub.status.busy":"2022-06-26T08:48:04.298740Z","iopub.execute_input":"2022-06-26T08:48:04.299552Z","iopub.status.idle":"2022-06-26T08:48:04.304299Z","shell.execute_reply.started":"2022-06-26T08:48:04.299514Z","shell.execute_reply":"2022-06-26T08:48:04.303174Z"}}},{"cell_type":"markdown","source":"**Exploratory data analysis** is a crucial process in Data analysis that enables us to gain insights into the data. We try to understand the data in details before pre-processing. ","metadata":{}},{"cell_type":"code","source":"#lets preview both the test and train data\nprint(f'shape == {train.shape}')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:31.937347Z","iopub.execute_input":"2022-06-26T10:52:31.937741Z","iopub.status.idle":"2022-06-26T10:52:31.961601Z","shell.execute_reply.started":"2022-06-26T10:52:31.937700Z","shell.execute_reply":"2022-06-26T10:52:31.959895Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#lets preview the first 5 entries of the test data\nprint(f'shape == {test.shape}')\ntest.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:31.965678Z","iopub.execute_input":"2022-06-26T10:52:31.965998Z","iopub.status.idle":"2022-06-26T10:52:31.980800Z","shell.execute_reply.started":"2022-06-26T10:52:31.965971Z","shell.execute_reply":"2022-06-26T10:52:31.979448Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#lets see our unique y variable\nlanguages = list(np.unique(np.array(train.lang_id.to_list())))\nlanguages","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:31.983698Z","iopub.execute_input":"2022-06-26T10:52:31.984117Z","iopub.status.idle":"2022-06-26T10:52:32.010563Z","shell.execute_reply.started":"2022-06-26T10:52:31.984060Z","shell.execute_reply":"2022-06-26T10:52:32.009512Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"It looks like we have 11 unique languages that will train our model on","metadata":{"execution":{"iopub.status.busy":"2022-06-26T09:23:33.876067Z","iopub.execute_input":"2022-06-26T09:23:33.876537Z","iopub.status.idle":"2022-06-26T09:23:33.886075Z","shell.execute_reply.started":"2022-06-26T09:23:33.876501Z","shell.execute_reply":"2022-06-26T09:23:33.883926Z"}}},{"cell_type":"code","source":"#wec then check how our y variable is distributed among the 11 languages\ntotal_dist = train.groupby(by = 'lang_id').count()\ntotal_dist","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:32.012702Z","iopub.execute_input":"2022-06-26T10:52:32.013104Z","iopub.status.idle":"2022-06-26T10:52:32.043594Z","shell.execute_reply.started":"2022-06-26T10:52:32.013066Z","shell.execute_reply":"2022-06-26T10:52:32.042528Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"So its true that our we wont need any upsampling or downsampling staff as our data is normally distributed among the 11 languages","metadata":{}},{"cell_type":"code","source":"#we plot a line graph with the count of languages on y axis\ntotal_dist.plot()\nplt.title('line graph showing count of languages')\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:32.045795Z","iopub.execute_input":"2022-06-26T10:52:32.046611Z","iopub.status.idle":"2022-06-26T10:52:32.294630Z","shell.execute_reply.started":"2022-06-26T10:52:32.046568Z","shell.execute_reply":"2022-06-26T10:52:32.293628Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"plt.pie(total_dist.text, labels = total_dist.index)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:32.296059Z","iopub.execute_input":"2022-06-26T10:52:32.297203Z","iopub.status.idle":"2022-06-26T10:52:32.458470Z","shell.execute_reply.started":"2022-06-26T10:52:32.297160Z","shell.execute_reply":"2022-06-26T10:52:32.456967Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"From the above two graphs, we can tell that our data in normally distributed","metadata":{}},{"cell_type":"code","source":"#we then print the first few information of our training set\nfor i in train.index:\n    if i < 5:\n        print(train.loc[i].text)\n   \n    elif i >32995:\n        print(train.loc[i].text)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:32.460927Z","iopub.execute_input":"2022-06-26T10:52:32.461372Z","iopub.status.idle":"2022-06-26T10:52:32.488998Z","shell.execute_reply.started":"2022-06-26T10:52:32.461327Z","shell.execute_reply":"2022-06-26T10:52:32.486415Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Lets have a look at the test data","metadata":{}},{"cell_type":"code","source":"#we preview the first 20 elements on the text entries\ntest.head(20).text.to_list()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:32.495786Z","iopub.execute_input":"2022-06-26T10:52:32.496298Z","iopub.status.idle":"2022-06-26T10:52:32.507665Z","shell.execute_reply.started":"2022-06-26T10:52:32.496243Z","shell.execute_reply":"2022-06-26T10:52:32.506078Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"From the above data We can note the following:\n> 1. There are punctuations to be dealt with here\n> 2. There is a unique tag (<fn>) That needs special attention","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:11:32.025646Z","iopub.execute_input":"2022-06-26T10:11:32.026157Z","iopub.status.idle":"2022-06-26T10:11:32.032910Z","shell.execute_reply.started":"2022-06-26T10:11:32.026120Z","shell.execute_reply":"2022-06-26T10:11:32.031781Z"}}},{"cell_type":"code","source":"#We collent the entries having fn tags for analysis\nhave_fn = []\nfor i in test.index:\n    txt = test.loc[i].text\n    if '<fn>' in txt:\n        have_fn.append(txt)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:32.510033Z","iopub.execute_input":"2022-06-26T10:52:32.510926Z","iopub.status.idle":"2022-06-26T10:52:33.268957Z","shell.execute_reply.started":"2022-06-26T10:52:32.510881Z","shell.execute_reply":"2022-06-26T10:52:33.267852Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#lets print the first two of those special cases\nhave_fn[:2]","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:33.270732Z","iopub.execute_input":"2022-06-26T10:52:33.271130Z","iopub.status.idle":"2022-06-26T10:52:33.278761Z","shell.execute_reply.started":"2022-06-26T10:52:33.271088Z","shell.execute_reply":"2022-06-26T10:52:33.277402Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#how many of those special cases are available\nfn_total = len(have_fn)\nfn_total","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:33.280428Z","iopub.execute_input":"2022-06-26T10:52:33.281409Z","iopub.status.idle":"2022-06-26T10:52:33.290240Z","shell.execute_reply.started":"2022-06-26T10:52:33.281365Z","shell.execute_reply":"2022-06-26T10:52:33.289125Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_total = test.shape[0]\nlabels = ['have fn tag','lack fn tag']\nvalues = [fn_total,(test_total-fn_total)]\nplt.pie(values, labels=labels, colors = ['red','green'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:33.292125Z","iopub.execute_input":"2022-06-26T10:52:33.293300Z","iopub.status.idle":"2022-06-26T10:52:33.391953Z","shell.execute_reply.started":"2022-06-26T10:52:33.293258Z","shell.execute_reply":"2022-06-26T10:52:33.390634Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#lets get the fn total percentage\nfail_percentage = np.round((fn_total/test_total)*100,2)\nfail_percentage","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:33.397599Z","iopub.execute_input":"2022-06-26T10:52:33.398614Z","iopub.status.idle":"2022-06-26T10:52:33.415686Z","shell.execute_reply.started":"2022-06-26T10:52:33.398561Z","shell.execute_reply":"2022-06-26T10:52:33.414353Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We have 1.9 percent of our data having fn tags. This implies that our model will havean f1 score of 98.1% maximum.\nTo solve that kind of problem, will pass our test data through a funtion which will separate the entries with <fn> tag with those lacking the tag.\n\nWill then handle those with the <fn> tag with special considerations","metadata":{}},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"markdown","source":"# FEATURE ENGENEERING","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:27:32.176579Z","iopub.execute_input":"2022-06-26T10:27:32.177118Z","iopub.status.idle":"2022-06-26T10:27:32.183658Z","shell.execute_reply.started":"2022-06-26T10:27:32.177083Z","shell.execute_reply":"2022-06-26T10:27:32.182428Z"}}},{"cell_type":"markdown","source":"We create a funtion called train_processing to process our train data before feeding it into the model","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:30:29.986146Z","iopub.execute_input":"2022-06-26T10:30:29.986685Z","iopub.status.idle":"2022-06-26T10:30:29.995873Z","shell.execute_reply.started":"2022-06-26T10:30:29.986649Z","shell.execute_reply":"2022-06-26T10:30:29.993833Z"}}},{"cell_type":"code","source":"def train_preprocessing(text):\n    \n    '''\n    This functions cleans text from line breaks, URLs, numbers, etc.\n    ''' \n    text = text.lower() #to lower case\n    text = text.replace('\\n', ' ') # remove line breaks\n    text = text.replace('\\@(\\w*)', '') # remove mentions\n    text = re.sub(r\"\\bhttps://t.co/\\w+\", '', text) # remove URLs\n    text = re.sub(r'\\#', '', text) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n    text = re.sub(' +', ' ', text) # remove 1+ spaces\n    text = re.sub(\"\\n\",\" \",text)\n    text =' '.join(text.split())\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:33.417767Z","iopub.execute_input":"2022-06-26T10:52:33.420903Z","iopub.status.idle":"2022-06-26T10:52:33.439744Z","shell.execute_reply.started":"2022-06-26T10:52:33.420852Z","shell.execute_reply":"2022-06-26T10:52:33.438105Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#we then apply the text processing to our train data\ntrain['text'] = train['text'].apply(train_preprocessing)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:56.481457Z","iopub.execute_input":"2022-06-26T10:52:56.481993Z","iopub.status.idle":"2022-06-26T10:52:58.260616Z","shell.execute_reply.started":"2022-06-26T10:52:56.481948Z","shell.execute_reply":"2022-06-26T10:52:58.259463Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#we devide our features into the x and y as follows\nX = train['text']\ny = train['lang_id']","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:52:58.959318Z","iopub.execute_input":"2022-06-26T10:52:58.960512Z","iopub.status.idle":"2022-06-26T10:52:58.969079Z","shell.execute_reply.started":"2022-06-26T10:52:58.960462Z","shell.execute_reply":"2022-06-26T10:52:58.967898Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"##### ","metadata":{}},{"cell_type":"markdown","source":"# MODEL BUILDING","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:33:08.779726Z","iopub.execute_input":"2022-06-26T10:33:08.780322Z","iopub.status.idle":"2022-06-26T10:33:08.787969Z","shell.execute_reply.started":"2022-06-26T10:33:08.780289Z","shell.execute_reply":"2022-06-26T10:33:08.785864Z"}}},{"cell_type":"markdown","source":"Will use train test split to devide our data into training and testing set. Will the try multiple models and then pick the one performing best on the testing data","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:34:28.790138Z","iopub.execute_input":"2022-06-26T10:34:28.790697Z","iopub.status.idle":"2022-06-26T10:34:28.799027Z","shell.execute_reply.started":"2022-06-26T10:34:28.790660Z","shell.execute_reply":"2022-06-26T10:34:28.797517Z"}}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T10:53:01.638814Z","iopub.execute_input":"2022-06-26T10:53:01.639199Z","iopub.status.idle":"2022-06-26T10:53:01.653222Z","shell.execute_reply.started":"2022-06-26T10:53:01.639166Z","shell.execute_reply":"2022-06-26T10:53:01.651905Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### MultinomialNB(Best model for this data set)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:09:42.030879Z","iopub.execute_input":"2022-06-26T11:09:42.031498Z","iopub.status.idle":"2022-06-26T11:09:42.041069Z","shell.execute_reply.started":"2022-06-26T11:09:42.031446Z","shell.execute_reply":"2022-06-26T11:09:42.038340Z"}}},{"cell_type":"markdown","source":"Will strart with the model of intrest aka MultinomialNB Then will also check other models and see how they also performs","metadata":{}},{"cell_type":"code","source":"# Creating a pipeline for the gridsearch\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'alpha': [0.047]}  # setting parameter grid\n\ntuned_mnb = Pipeline([('tfidf', TfidfVectorizer(\n                                                max_df=0.9,ngram_range=(1, 5), analyzer= 'char'\n                                               )),\n                      ('mnb', GridSearchCV(MultinomialNB(),\n                                           param_grid=param_grid,\n                                           cv=5,\n                                           scoring='f1_weighted'))\n                      ])\n\ntuned_mnb.fit(X_train, y_train)  # Fitting the model\n\ny_pred_mnb = tuned_mnb.predict(X_val)  # predicting the fit on validation set\n\nprint(classification_report(y_val, y_pred_mnb))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:06:30.486476Z","iopub.execute_input":"2022-06-26T11:06:30.487348Z","iopub.status.idle":"2022-06-26T11:07:05.365782Z","shell.execute_reply.started":"2022-06-26T11:06:30.487300Z","shell.execute_reply":"2022-06-26T11:07:05.364575Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Other models","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:10:04.390936Z","iopub.execute_input":"2022-06-26T11:10:04.391291Z","iopub.status.idle":"2022-06-26T11:10:04.397906Z","shell.execute_reply.started":"2022-06-26T11:10:04.391262Z","shell.execute_reply":"2022-06-26T11:10:04.396321Z"}}},{"cell_type":"code","source":"# Random Forest Classifier\nrf = Pipeline([('tfidf', TfidfVectorizer(max_df=0.9,min_df = 2,ngram_range=(1, 1), analyzer= 'word')),\n               ('clf', RandomForestClassifier(max_depth=100, \n                                              n_estimators=100))])\n\n# NaÃ¯ve Bayes:\nnb = Pipeline([('tfidf', TfidfVectorizer(max_df=0.9,min_df = 2,ngram_range=(1, 1), analyzer= 'word')),\n               ('clf', MultinomialNB())])\n\n# K-NN Classifier\nknn = Pipeline([('tfidf', TfidfVectorizer(max_df=0.9,min_df = 2,ngram_range=(1, 1), analyzer= 'word')),\n                ('clf', KNeighborsClassifier(n_neighbors=5,  \n                                             p=2))])\n\n# Logistic Regression\nlr = Pipeline([('tfidf',TfidfVectorizer(max_df=0.9,min_df = 2,ngram_range=(1, 1), analyzer= 'word')),\n               ('clf',LogisticRegression(C=1,  \n                                         max_iter=1000))])\n# Linear SVC:\nlsvc = Pipeline([('tfidf', TfidfVectorizer(max_df=0.9,min_df = 2,ngram_range=(1, 1), analyzer= 'word')),\n                 ('clf', LinearSVC())])","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:17:01.458477Z","iopub.execute_input":"2022-06-26T11:17:01.459044Z","iopub.status.idle":"2022-06-26T11:17:01.479361Z","shell.execute_reply.started":"2022-06-26T11:17:01.459004Z","shell.execute_reply":"2022-06-26T11:17:01.476667Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Fiting other models ","metadata":{}},{"cell_type":"code","source":"# Random forest \nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_val)\n\n# Niave bayes\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_val)\n\n# K - nearest neighbors\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_val)\n\n# Linear regression\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_val)\n\n# Linear SVC\nlsvc.fit(X_train, y_train)\ny_pred_lsvc = lsvc.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:17:52.870184Z","iopub.execute_input":"2022-06-26T11:17:52.870521Z","iopub.status.idle":"2022-06-26T11:18:56.386800Z","shell.execute_reply.started":"2022-06-26T11:17:52.870495Z","shell.execute_reply":"2022-06-26T11:18:56.385645Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"print('Random forest classifier')\nprint(classification_report(y_val, y_pred_rf))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:20:20.504801Z","iopub.execute_input":"2022-06-26T11:20:20.505446Z","iopub.status.idle":"2022-06-26T11:20:20.531086Z","shell.execute_reply.started":"2022-06-26T11:20:20.505411Z","shell.execute_reply":"2022-06-26T11:20:20.529924Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print('Naive Bayes classifier')\nprint(classification_report(y_val, y_pred_nb))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:20:26.075138Z","iopub.execute_input":"2022-06-26T11:20:26.075511Z","iopub.status.idle":"2022-06-26T11:20:26.097251Z","shell.execute_reply.started":"2022-06-26T11:20:26.075479Z","shell.execute_reply":"2022-06-26T11:20:26.096243Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print('K-Nearest Neighbours classifier')\nprint(classification_report(y_val, y_pred_knn))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:20:32.271856Z","iopub.execute_input":"2022-06-26T11:20:32.272236Z","iopub.status.idle":"2022-06-26T11:20:32.299520Z","shell.execute_reply.started":"2022-06-26T11:20:32.272204Z","shell.execute_reply":"2022-06-26T11:20:32.298287Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"print('Logistic linear Regression')\nprint(classification_report(y_val, y_pred_lr))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:21:09.580205Z","iopub.execute_input":"2022-06-26T11:21:09.581281Z","iopub.status.idle":"2022-06-26T11:21:09.606943Z","shell.execute_reply.started":"2022-06-26T11:21:09.581214Z","shell.execute_reply":"2022-06-26T11:21:09.605713Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print('Linear SVC')\nprint(classification_report(y_val, y_pred_lsvc))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:21:50.194100Z","iopub.execute_input":"2022-06-26T11:21:50.194466Z","iopub.status.idle":"2022-06-26T11:21:50.220354Z","shell.execute_reply.started":"2022-06-26T11:21:50.194436Z","shell.execute_reply":"2022-06-26T11:21:50.219236Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Conclussion","metadata":{}},{"cell_type":"markdown","source":"Even Though MultinomialNB is the best model of choice, Other models when properly tuned can also perform absolutely well","metadata":{}},{"cell_type":"markdown","source":"# Kaggle submission","metadata":{}},{"cell_type":"markdown","source":"We wont just call .predict on our test data but rather, will do the following:\n> 1. Will clean our test data\n> 2. Will create a funtion to separate the the special case ie <fn> with the non-special case\n> 3. Will create a funtion to predict the special case and will use  .predict to predict the non-special case","metadata":{}},{"cell_type":"code","source":"# test['text'] = test['text'].apply(train_preprocessing)\n# submission_trial = pd.DataFrame(test['index'])\n# submission_trial['lang_id'] = tuned_mnb.predict(test['text'])\n# submission_trial.to_csv('g.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T11:07:14.574375Z","iopub.execute_input":"2022-06-26T11:07:14.574730Z","iopub.status.idle":"2022-06-26T11:07:18.275122Z","shell.execute_reply.started":"2022-06-26T11:07:14.574702Z","shell.execute_reply":"2022-06-26T11:07:18.273826Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}