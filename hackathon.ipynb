{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e389f07",
   "metadata": {},
   "source": [
    "## Hackathon challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fd15a",
   "metadata": {},
   "source": [
    "Will start by importing the required libraries to help in our text processing and loading datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136e39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "#nltk.download(['stopwords','punkt']) will pass stopwords to the tfidvectorizer\n",
    "#from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80a82c",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89fd01",
   "metadata": {},
   "source": [
    "# EDA (EXPLORATORY DATA ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0d6d5",
   "metadata": {},
   "source": [
    "The training and testing data are loaded for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931280c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_set.csv')\n",
    "test = pd.read_csv('./test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a81bc",
   "metadata": {},
   "source": [
    "Will check the content of both the training and testing data to have an overview of what will be dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d55699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first five features in the training deata\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fa7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frist five features in the testing data\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e6a4d",
   "metadata": {},
   "source": [
    "Will then check the shape of our dataset. This helps in choosing the right model as some models might not do well small datasets while others might do well in large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9bc2ed",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a48a6",
   "metadata": {},
   "source": [
    "# Feature engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937631bc",
   "metadata": {},
   "source": [
    "We first start by defining a funtion to clean our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ffeb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2022-06-24 20:05:17 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    '''\n",
    "    This function cleans text \n",
    "    '''\n",
    "    \n",
    "    text = text.lower() #to lower case\n",
    "    text = text.replace('\\n', ' ') # remove line breaks\n",
    "    text = text.replace('\\@(\\w*)', '') # remove mentions\n",
    "    text = re.sub(r\"\\bhttps://t.co/\\w+\", '', text) # remove URLs\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # remove numbers\n",
    "    text = re.sub(r'\\#', '', text) # remove hashtags. To remove full hashtag: '\\#(\\w*)'\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # removes numbers?\n",
    "    text = re.sub(' +', ' ', text) # remove 1+ spaces\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    text =' '.join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02782f3e",
   "metadata": {},
   "source": [
    "We apply the above funtion to both the train and testing datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283ec2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.92 s (started: 2022-06-24 20:05:18 +01:00)\n"
     ]
    }
   ],
   "source": [
    "train['text'] = train['text'].apply(text_preprocessing)\n",
    "test['text'] = test['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f07b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSON\\AppData\\Local\\Temp/ipykernel_17924/2305434871.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 406 ms (started: 2022-06-24 20:05:28 +01:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAMSON\\AppData\\Local\\Temp/ipykernel_17924/2305434871.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")\n"
     ]
    }
   ],
   "source": [
    "# Replace '.txt' with 'text file'\n",
    "train[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\n",
    "test[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca824ac9",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e75f9",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e197e38",
   "metadata": {},
   "source": [
    "We separate our training dataset into X and Y awaiting model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07b2fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2022-06-24 20:05:28 +01:00)\n"
     ]
    }
   ],
   "source": [
    "X = train['text']\n",
    "y = train['lang_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459260a",
   "metadata": {},
   "source": [
    "We then split the data into the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5874df48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2022-06-24 23:26:47 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Refining the train-test split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01)#test size of 0.01 was used only fore the purpose of hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258fdb9",
   "metadata": {},
   "source": [
    "We then fit our model and check the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fab0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         afr       1.00      1.00      1.00        26\n",
      "         eng       1.00      1.00      1.00        23\n",
      "         nbl       1.00      1.00      1.00        23\n",
      "         nso       1.00      1.00      1.00        37\n",
      "         sot       1.00      1.00      1.00        33\n",
      "         ssw       1.00      1.00      1.00        31\n",
      "         tsn       1.00      1.00      1.00        35\n",
      "         tso       1.00      1.00      1.00        23\n",
      "         ven       1.00      1.00      1.00        29\n",
      "         xho       1.00      1.00      1.00        32\n",
      "         zul       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00       330\n",
      "   macro avg       1.00      1.00      1.00       330\n",
      "weighted avg       1.00      1.00      1.00       330\n",
      "\n",
      "time: 7.14 s (started: 2022-06-24 23:38:09 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Creating a pipeline for the gridsearch\n",
    "param_grid = {'alpha': [0.1, 0.01,0.001]}  # alpha value of below 2 gives the best f1 score\n",
    "\n",
    "tuned_mnb = Pipeline([('tfidf', TfidfVectorizer(min_df=2,\n",
    "                                                max_df=0.9,\n",
    "                                                ngram_range=(1, 1))),\n",
    "                      ('mnb', GridSearchCV(MultinomialNB(),\n",
    "                                           param_grid=param_grid,\n",
    "                                           cv=6,\n",
    "                                           scoring='f1_weighted'))\n",
    "                      ])\n",
    "\n",
    "tuned_mnb.fit(X_train, y_train)  # Fitting the model\n",
    "\n",
    "y_pred_mnb = tuned_mnb.predict(X_val)  # predicting the fit on validation set\n",
    "\n",
    "print(classification_report(y_val, y_pred_mnb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2eae2",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673d6bc",
   "metadata": {},
   "source": [
    "# MAKING SUBMISSION TO KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a3b5f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 407 ms (started: 2022-06-24 23:38:27 +01:00)\n"
     ]
    }
   ],
   "source": [
    "submission_trial = pd.DataFrame(test['index'])\n",
    "submission_trial['lang_id'] = tuned_mnb.predict(test['text'])\n",
    "submission_trial.to_csv('hackathon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bd426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
